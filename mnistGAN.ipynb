{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUBkgNFPwxhuHGV659xHzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtroyv/generative-adversarial-networks/blob/main/mnistGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uENlF0Uw2N0n",
        "outputId": "c3764cea-2688-47c2-c917-5917e4c2a28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "GPU Available:  True\n",
            "cuda:0\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"GPU Available: \", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device= torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device=\"cpu\"\n",
        "print(device)\n",
        "\n",
        "# If you want to save the model to your personal google drive or transfer files you need to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to create out generator\n",
        "def make_generator_network(input_size=20, num_hidden_layers=1, num_hidden_units=100, num_output_units=784):\n",
        "  model = nn.Sequential()\n",
        "  for i in range(num_hidden_layers):\n",
        "    model.add_module(f'fc_g{i}', nn.Linear(input_size, num_hidden_units))\n",
        "    model.add_module(f'relu_g{i}', nn.LeakyReLU())\n",
        "    input_size = num_hidden_units\n",
        "  model.add_module(f'fc_g{num_hidden_layers}', nn.Linear(input_size, num_output_units))\n",
        "  model.add_module('tanh_g', nn.Tanh())\n",
        "  return model\n",
        "\n",
        "#Define a function for the descriminator\n",
        "def make_descriminator_network(input_size, num_hidden_layers=1, num_hidden_units=100, num_output_units =1):\n",
        "  model = nn.Sequential()\n",
        "  for i in range(num_hidden_layers):\n",
        "    model.add_module(f'fc_d{i}', nn.Linear(input_size, num_hidden_units, bias=False))\n",
        "    model.add_module(f'relu_d{i}', nn.LeakyReLU())\n",
        "    model.add_module(f'dropout', nn.Dropout(p=0.5))\n",
        "    input_size= num_hidden_units\n",
        "  model.add_module(f'fc_d{num_hidden_layers}', nn.Linear(input_size, num_output_units))\n",
        "  model.add_module('sigmoid', nn.Sigmoid())\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "#Next we initialize each of the networks\n",
        "image_size= (28,28)\n",
        "z_size = 20\n",
        "gen_hidden_layers = 1\n",
        "gen_hidden_size = 100\n",
        "disc_hidden_layers =1\n",
        "disc_hidden_size=100\n",
        "torch.manual_seed(1)\n",
        "\n",
        "gen_model = make_generator_network(input_size=z_size, num_hidden_layers=gen_hidden_layers, num_hidden_units=gen_hidden_size, num_output_units=np.prod(image_size))\n",
        "print(gen_model)\n",
        "\n",
        "disc_model = make_descriminator_network(input_size=np.prod(image_size), num_hidden_layers=disc_hidden_layers, num_hidden_units=disc_hidden_size)\n",
        "print(disc_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FddazO2E-Gno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217f9806-fe04-4424-8d93-a417c9359c44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (fc_g0): Linear(in_features=20, out_features=100, bias=True)\n",
            "  (relu_g0): LeakyReLU(negative_slope=0.01)\n",
            "  (fc_g1): Linear(in_features=100, out_features=784, bias=True)\n",
            "  (tanh_g): Tanh()\n",
            ")\n",
            "Sequential(\n",
            "  (fc_d0): Linear(in_features=784, out_features=100, bias=False)\n",
            "  (relu_d0): LeakyReLU(negative_slope=0.01)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc_d1): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "image_path='./'\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5), std=(0.5)) ])\n",
        "\n",
        "mnist_dataset= torchvision.datasets.MNIST(root=image_path, train=True, transform = transform, download=False)\n",
        "\n",
        "example,label = next(iter(mnist_dataset))\n",
        "print(f'Min: {example.min()} Max: {example.max()}')\n",
        "print(example.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSEmUPczZUbS",
        "outputId": "c6932254-82b9-4065-a296-439ffdfc53da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: -1.0 Max: 1.0\n",
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random vector z based on desired distribution\n",
        "\n",
        "def create_noise(batch_size, z_size, mode_z):\n",
        "  if mode_z =='uniform':\n",
        "    input_z = torch.rand(batch_size, z_size) *2 -1 #why do we combine it like this?\n",
        "  elif mode_z == 'normal':\n",
        "    input_z = torch.randn(batch_size, z_size)\n",
        "  return input_z"
      ],
      "metadata": {
        "id": "dJb547FAh2UK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run through and get a batch of probabilities for the fake images and real images\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(mnist_dataset, batch_size, shuffle=False)\n",
        "input_real, label = next(iter(dataloader))\n",
        "input_real = input_real.view(batch_size, -1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "mode_z ='uniform'\n",
        "input_z = create_noise(batch_size, z_size, mode_z)\n",
        "print('input-z -- shape:', input_z.shape)\n",
        "print('input-real -- shape:', input_real.shape)\n",
        "\n",
        "g_output = gen_model(input_z)\n",
        "print('Output of G -- shape:', g_output.shape)\n",
        "\n",
        "d_proba_real = disc_model(input_real)\n",
        "d_proba_fake = disc_model(g_output)\n",
        "print('Probability of real images -- shape:', d_proba_real.shape)\n",
        "print('Probability of fake images -- shape:', d_proba_fake.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPqKnE6Sml5W",
        "outputId": "12837fe2-2188-4fb4-ed8e-86e63c8614e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input-z -- shape: torch.Size([32, 20])\n",
            "input-real -- shape: torch.Size([32, 784])\n",
            "Output of G -- shape: torch.Size([32, 784])\n",
            "Probability of real images -- shape: torch.Size([32, 1])\n",
            "Probability of fake images -- shape: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create loss and generate ground truth labels\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "# loss for generator\n",
        "g_labels_real = torch.ones_like(d_proba_fake)\n",
        "g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "print(f'Generator Loss: {g_loss:.4f}')\n",
        "\n",
        "# Loss for discriminator\n",
        "d_labels_real = torch.ones_like(d_proba_real)\n",
        "d_labels_fake = torch.zeros_like(d_proba_fake)\n",
        "d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "print(f'Discriminator losses: Real {d_loss_real:.4f} Fake {d_loss_fake:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ-hN9y_80Dd",
        "outputId": "8302adcc-6262-4960-d6f6-a6e4ea815372"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.6983\n",
            "Discriminator losses: Real 0.7479 Fake 0.6885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we set up teh dataloader for the real dataset, the generator, discriminator and separate adam optimizer\n",
        "\n",
        "batch_size = 64\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "mnist_dl =DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "first_images, first_labels = next(iter(mnist_dl))\n",
        "print(f'The first image is {first_images[0].shape}')\n",
        "print(f\"THe first label is {first_labels[0]}\")\n",
        "\n",
        "gen_model = make_generator_network(\n",
        "    input_size=z_size,\n",
        "    num_hidden_layers=gen_hidden_layers,\n",
        "    num_hidden_units= gen_hidden_size,\n",
        "    num_output_units=np.prod(image_size)\n",
        ").to(device)\n",
        "\n",
        "disc_model = make_descriminator_network(\n",
        "    input_size=np.prod(image_size),\n",
        "    num_hidden_layers=disc_hidden_layers,\n",
        "    num_hidden_units=disc_hidden_size\n",
        ").to(device)\n",
        "\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "g_optimizer= torch.optim.Adam(gen_model.parameters())\n",
        "d_optimizer=torch.optim.Adam(disc_model.parameters())\n",
        "\n"
      ],
      "metadata": {
        "id": "Ze32xti_FI1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ab25f8-0158-4d1c-d353-35af94b05064"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first image is torch.Size([1, 28, 28])\n",
            "THe first label is 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write two training utility functions for the  discriminator and generator using two separate adam optimizers\n",
        "\n",
        "# Train the discriminator\n",
        "def d_train(x):\n",
        "  disc_model.zero_grad()\n",
        "  # Train the discriminator with a real batch\n",
        "  batch_size = x.size(0)\n",
        "  x = x.view(batch_size, -1).to(device)\n",
        "  d_labels_real = torch.ones(batch_size, 1, device=device)\n",
        "  d_proba_real = disc_model(x)\n",
        "  d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "\n",
        "  # Train discriminator on a fake batch\n",
        "\n",
        "  input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "  g_output = gen_model(input_z)\n",
        "  d_proba_fake = disc_model(g_output)\n",
        "  d_labels_fake = torch.zeros(batch_size,1,device=device)\n",
        "  d_loss_fake =loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "  # Gradient backprop & optimize only D's parameters\n",
        "  d_loss = d_loss_real + d_loss_fake\n",
        "  d_loss.backward()\n",
        "  d_optimizer.step()\n",
        "  return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()\n",
        "\n",
        "  #Train the generator\n",
        "\n",
        "def g_train(x):\n",
        "  gen_model.zero_grad()\n",
        "  batch_size = x.size(0)\n",
        "  input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "  g_labels_real = torch.ones(batch_size,1, device=device)\n",
        "\n",
        "  g_output= gen_model(input_z)\n",
        "  d_proba_fake = disc_model(g_output)\n",
        "  g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "  # gradient backprop & optimize ONLY G's parameters\n",
        "  g_loss.backward()\n",
        "  g_optimizer.step()\n",
        "  return g_loss.data.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vzRUsWtuGiIA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will start alternatinig traing of geneator and discriminator over 100 epochs\n",
        "# for each epoch we will record the loss for the generator, discriminator, aswell as loss for real & fake data respectively.\n",
        "# We will also generatre some examples from a the latent vector z from the current generator model via create_samples() and store these synthnesized images in a list\n",
        "\n",
        "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "  g_output = g_model(input_z)\n",
        "  images =torch.reshape(g_output, (batch_size, *image_size))\n",
        "  return (images+1)/2.0\n",
        "\n",
        "epoch_samples = []\n",
        "all_d_losses = []\n",
        "all_g_losses = []\n",
        "all_d_real = []\n",
        "all_d_fake = []\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  d_losses, g_losses = [],[]\n",
        "\n",
        "  d_vals_real, d_vals_fake = [],[]\n",
        "\n",
        "  for i, (x,_) in enumerate(mnist_dl):\n",
        "    d_loss, d_proba_real, d_proba_fake= d_train(x)\n",
        "    d_losses.append(d_loss)\n",
        "    g_losses.append(g_train(x))\n",
        "    d_vals_real.append(d_proba_real.mean().cpu())\n",
        "    d_vals_fake.append(d_proba_fake.mean().cpu())\n",
        "\n",
        "  all_d_losses.append(torch.tensor(d_losses).mean())\n",
        "  all_g_losses.append(torch.tensor(g_losses).mean())\n",
        "  all_d_real.append(torch.tensor(d_vals_real).mean())\n",
        "  all_d_fake.append(torch.tensor(d_vals_fake).mean())\n",
        "  print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
        "  f'G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:4f}'\n",
        "  f' [D-Real: {all_d_real[-1]:.4f}'\n",
        "  f' D-Fake: {all_d_fake[-1]:.4f}]')\n",
        "  epoch_samples.append(create_samples(gen_model, fixed_z).detach().cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsrJTDjq1Fz5",
        "outputId": "5101d0fe-b722-403a-8d07-94bbb0997ba2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Avg Losses >>G/D 0.9020/0.880472 [D-Real: 0.8317 D-Fake: 0.4781]\n",
            "Epoch 002 | Avg Losses >>G/D 1.0007/1.092743 [D-Real: 0.6307 D-Fake: 0.4228]\n",
            "Epoch 003 | Avg Losses >>G/D 0.9675/1.160848 [D-Real: 0.5989 D-Fake: 0.4183]\n",
            "Epoch 004 | Avg Losses >>G/D 0.9796/1.194591 [D-Real: 0.5832 D-Fake: 0.4180]\n",
            "Epoch 005 | Avg Losses >>G/D 0.9081/1.233730 [D-Real: 0.5686 D-Fake: 0.4323]\n",
            "Epoch 006 | Avg Losses >>G/D 0.9623/1.215204 [D-Real: 0.5723 D-Fake: 0.4205]\n",
            "Epoch 007 | Avg Losses >>G/D 0.8827/1.266847 [D-Real: 0.5527 D-Fake: 0.4431]\n",
            "Epoch 008 | Avg Losses >>G/D 1.0358/1.180521 [D-Real: 0.5917 D-Fake: 0.4089]\n",
            "Epoch 009 | Avg Losses >>G/D 1.1428/1.095757 [D-Real: 0.6252 D-Fake: 0.3808]\n",
            "Epoch 010 | Avg Losses >>G/D 0.9542/1.197297 [D-Real: 0.5876 D-Fake: 0.4184]\n",
            "Epoch 011 | Avg Losses >>G/D 0.9545/1.205703 [D-Real: 0.5836 D-Fake: 0.4190]\n",
            "Epoch 012 | Avg Losses >>G/D 0.9768/1.206133 [D-Real: 0.5817 D-Fake: 0.4169]\n",
            "Epoch 013 | Avg Losses >>G/D 0.9669/1.204329 [D-Real: 0.5821 D-Fake: 0.4180]\n",
            "Epoch 014 | Avg Losses >>G/D 0.9616/1.207925 [D-Real: 0.5826 D-Fake: 0.4199]\n",
            "Epoch 015 | Avg Losses >>G/D 0.9219/1.237280 [D-Real: 0.5698 D-Fake: 0.4308]\n",
            "Epoch 016 | Avg Losses >>G/D 0.9098/1.241003 [D-Real: 0.5684 D-Fake: 0.4352]\n",
            "Epoch 017 | Avg Losses >>G/D 0.8867/1.260598 [D-Real: 0.5596 D-Fake: 0.4406]\n",
            "Epoch 018 | Avg Losses >>G/D 0.8449/1.292415 [D-Real: 0.5451 D-Fake: 0.4527]\n",
            "Epoch 019 | Avg Losses >>G/D 0.8154/1.307110 [D-Real: 0.5378 D-Fake: 0.4605]\n",
            "Epoch 020 | Avg Losses >>G/D 0.8250/1.311625 [D-Real: 0.5365 D-Fake: 0.4611]\n",
            "Epoch 021 | Avg Losses >>G/D 0.8424/1.291331 [D-Real: 0.5445 D-Fake: 0.4536]\n",
            "Epoch 022 | Avg Losses >>G/D 0.8433/1.291119 [D-Real: 0.5473 D-Fake: 0.4556]\n",
            "Epoch 023 | Avg Losses >>G/D 0.8527/1.280839 [D-Real: 0.5499 D-Fake: 0.4511]\n",
            "Epoch 024 | Avg Losses >>G/D 0.8169/1.306358 [D-Real: 0.5387 D-Fake: 0.4611]\n",
            "Epoch 025 | Avg Losses >>G/D 0.8397/1.294256 [D-Real: 0.5458 D-Fake: 0.4563]\n",
            "Epoch 026 | Avg Losses >>G/D 0.7983/1.317690 [D-Real: 0.5341 D-Fake: 0.4666]\n",
            "Epoch 027 | Avg Losses >>G/D 0.8025/1.321376 [D-Real: 0.5318 D-Fake: 0.4660]\n",
            "Epoch 028 | Avg Losses >>G/D 0.8109/1.311642 [D-Real: 0.5361 D-Fake: 0.4635]\n",
            "Epoch 029 | Avg Losses >>G/D 0.8415/1.290754 [D-Real: 0.5460 D-Fake: 0.4550]\n",
            "Epoch 030 | Avg Losses >>G/D 0.8239/1.300653 [D-Real: 0.5423 D-Fake: 0.4593]\n",
            "Epoch 031 | Avg Losses >>G/D 0.8009/1.314518 [D-Real: 0.5348 D-Fake: 0.4652]\n",
            "Epoch 032 | Avg Losses >>G/D 0.8160/1.308235 [D-Real: 0.5386 D-Fake: 0.4613]\n",
            "Epoch 033 | Avg Losses >>G/D 0.7959/1.321635 [D-Real: 0.5322 D-Fake: 0.4678]\n",
            "Epoch 034 | Avg Losses >>G/D 0.7896/1.332059 [D-Real: 0.5273 D-Fake: 0.4713]\n",
            "Epoch 035 | Avg Losses >>G/D 0.8106/1.315635 [D-Real: 0.5345 D-Fake: 0.4647]\n",
            "Epoch 036 | Avg Losses >>G/D 0.7831/1.327964 [D-Real: 0.5290 D-Fake: 0.4714]\n",
            "Epoch 037 | Avg Losses >>G/D 0.7855/1.326026 [D-Real: 0.5297 D-Fake: 0.4697]\n",
            "Epoch 038 | Avg Losses >>G/D 0.8040/1.310871 [D-Real: 0.5378 D-Fake: 0.4654]\n",
            "Epoch 039 | Avg Losses >>G/D 0.8120/1.309466 [D-Real: 0.5389 D-Fake: 0.4628]\n",
            "Epoch 040 | Avg Losses >>G/D 0.8026/1.326465 [D-Real: 0.5301 D-Fake: 0.4664]\n",
            "Epoch 041 | Avg Losses >>G/D 0.7780/1.334654 [D-Real: 0.5252 D-Fake: 0.4731]\n",
            "Epoch 042 | Avg Losses >>G/D 0.8033/1.314497 [D-Real: 0.5357 D-Fake: 0.4653]\n",
            "Epoch 043 | Avg Losses >>G/D 0.8026/1.313767 [D-Real: 0.5356 D-Fake: 0.4646]\n",
            "Epoch 044 | Avg Losses >>G/D 0.8086/1.315317 [D-Real: 0.5341 D-Fake: 0.4641]\n",
            "Epoch 045 | Avg Losses >>G/D 0.8288/1.298011 [D-Real: 0.5439 D-Fake: 0.4597]\n",
            "Epoch 046 | Avg Losses >>G/D 0.8160/1.301834 [D-Real: 0.5412 D-Fake: 0.4608]\n",
            "Epoch 047 | Avg Losses >>G/D 0.8215/1.300598 [D-Real: 0.5419 D-Fake: 0.4586]\n",
            "Epoch 048 | Avg Losses >>G/D 0.8349/1.297420 [D-Real: 0.5440 D-Fake: 0.4570]\n",
            "Epoch 049 | Avg Losses >>G/D 0.7952/1.323006 [D-Real: 0.5317 D-Fake: 0.4674]\n",
            "Epoch 050 | Avg Losses >>G/D 0.8009/1.311454 [D-Real: 0.5365 D-Fake: 0.4653]\n",
            "Epoch 051 | Avg Losses >>G/D 0.8077/1.310214 [D-Real: 0.5374 D-Fake: 0.4637]\n",
            "Epoch 052 | Avg Losses >>G/D 0.7940/1.322573 [D-Real: 0.5325 D-Fake: 0.4691]\n",
            "Epoch 053 | Avg Losses >>G/D 0.7948/1.324852 [D-Real: 0.5305 D-Fake: 0.4678]\n",
            "Epoch 054 | Avg Losses >>G/D 0.8106/1.312091 [D-Real: 0.5364 D-Fake: 0.4639]\n",
            "Epoch 055 | Avg Losses >>G/D 0.7833/1.332234 [D-Real: 0.5270 D-Fake: 0.4718]\n",
            "Epoch 056 | Avg Losses >>G/D 0.7977/1.321148 [D-Real: 0.5321 D-Fake: 0.4662]\n",
            "Epoch 057 | Avg Losses >>G/D 0.7847/1.326144 [D-Real: 0.5304 D-Fake: 0.4712]\n",
            "Epoch 058 | Avg Losses >>G/D 0.7927/1.322942 [D-Real: 0.5319 D-Fake: 0.4692]\n",
            "Epoch 059 | Avg Losses >>G/D 0.7961/1.322255 [D-Real: 0.5317 D-Fake: 0.4680]\n",
            "Epoch 060 | Avg Losses >>G/D 0.7842/1.330835 [D-Real: 0.5275 D-Fake: 0.4712]\n",
            "Epoch 061 | Avg Losses >>G/D 0.8013/1.317211 [D-Real: 0.5343 D-Fake: 0.4667]\n",
            "Epoch 062 | Avg Losses >>G/D 0.7845/1.329724 [D-Real: 0.5289 D-Fake: 0.4710]\n",
            "Epoch 063 | Avg Losses >>G/D 0.8010/1.315999 [D-Real: 0.5349 D-Fake: 0.4664]\n",
            "Epoch 064 | Avg Losses >>G/D 0.8036/1.312836 [D-Real: 0.5355 D-Fake: 0.4634]\n",
            "Epoch 065 | Avg Losses >>G/D 0.8013/1.323689 [D-Real: 0.5315 D-Fake: 0.4678]\n",
            "Epoch 066 | Avg Losses >>G/D 0.8112/1.312955 [D-Real: 0.5364 D-Fake: 0.4645]\n",
            "Epoch 067 | Avg Losses >>G/D 0.8129/1.312925 [D-Real: 0.5362 D-Fake: 0.4642]\n",
            "Epoch 068 | Avg Losses >>G/D 0.7922/1.328716 [D-Real: 0.5283 D-Fake: 0.4693]\n",
            "Epoch 069 | Avg Losses >>G/D 0.7882/1.329147 [D-Real: 0.5284 D-Fake: 0.4704]\n",
            "Epoch 070 | Avg Losses >>G/D 0.7878/1.331537 [D-Real: 0.5273 D-Fake: 0.4701]\n",
            "Epoch 071 | Avg Losses >>G/D 0.7813/1.330243 [D-Real: 0.5283 D-Fake: 0.4718]\n",
            "Epoch 072 | Avg Losses >>G/D 0.7803/1.334838 [D-Real: 0.5271 D-Fake: 0.4732]\n",
            "Epoch 073 | Avg Losses >>G/D 0.7713/1.340612 [D-Real: 0.5233 D-Fake: 0.4757]\n",
            "Epoch 074 | Avg Losses >>G/D 0.7807/1.335282 [D-Real: 0.5268 D-Fake: 0.4733]\n",
            "Epoch 075 | Avg Losses >>G/D 0.7722/1.337937 [D-Real: 0.5245 D-Fake: 0.4751]\n",
            "Epoch 076 | Avg Losses >>G/D 0.7732/1.337813 [D-Real: 0.5252 D-Fake: 0.4758]\n",
            "Epoch 077 | Avg Losses >>G/D 0.7742/1.342234 [D-Real: 0.5228 D-Fake: 0.4767]\n",
            "Epoch 078 | Avg Losses >>G/D 0.7951/1.324303 [D-Real: 0.5323 D-Fake: 0.4707]\n",
            "Epoch 079 | Avg Losses >>G/D 0.7737/1.326571 [D-Real: 0.5302 D-Fake: 0.4739]\n",
            "Epoch 080 | Avg Losses >>G/D 0.7886/1.331621 [D-Real: 0.5276 D-Fake: 0.4714]\n",
            "Epoch 081 | Avg Losses >>G/D 0.7887/1.328805 [D-Real: 0.5295 D-Fake: 0.4709]\n",
            "Epoch 082 | Avg Losses >>G/D 0.7637/1.343893 [D-Real: 0.5214 D-Fake: 0.4773]\n",
            "Epoch 083 | Avg Losses >>G/D 0.7703/1.345306 [D-Real: 0.5215 D-Fake: 0.4770]\n",
            "Epoch 084 | Avg Losses >>G/D 0.7642/1.342408 [D-Real: 0.5222 D-Fake: 0.4773]\n",
            "Epoch 085 | Avg Losses >>G/D 0.7710/1.339739 [D-Real: 0.5239 D-Fake: 0.4756]\n",
            "Epoch 086 | Avg Losses >>G/D 0.7647/1.342160 [D-Real: 0.5228 D-Fake: 0.4783]\n",
            "Epoch 087 | Avg Losses >>G/D 0.7780/1.337825 [D-Real: 0.5248 D-Fake: 0.4740]\n",
            "Epoch 088 | Avg Losses >>G/D 0.7936/1.327857 [D-Real: 0.5301 D-Fake: 0.4713]\n",
            "Epoch 089 | Avg Losses >>G/D 0.7940/1.325294 [D-Real: 0.5307 D-Fake: 0.4697]\n",
            "Epoch 090 | Avg Losses >>G/D 0.7721/1.337868 [D-Real: 0.5236 D-Fake: 0.4748]\n",
            "Epoch 091 | Avg Losses >>G/D 0.7794/1.337672 [D-Real: 0.5253 D-Fake: 0.4738]\n",
            "Epoch 092 | Avg Losses >>G/D 0.7713/1.339555 [D-Real: 0.5240 D-Fake: 0.4761]\n",
            "Epoch 093 | Avg Losses >>G/D 0.7868/1.331999 [D-Real: 0.5289 D-Fake: 0.4728]\n",
            "Epoch 094 | Avg Losses >>G/D 0.7696/1.344829 [D-Real: 0.5223 D-Fake: 0.4776]\n",
            "Epoch 095 | Avg Losses >>G/D 0.7708/1.338978 [D-Real: 0.5244 D-Fake: 0.4761]\n",
            "Epoch 096 | Avg Losses >>G/D 0.7663/1.343530 [D-Real: 0.5227 D-Fake: 0.4779]\n",
            "Epoch 097 | Avg Losses >>G/D 0.7905/1.329365 [D-Real: 0.5293 D-Fake: 0.4711]\n",
            "Epoch 098 | Avg Losses >>G/D 0.7904/1.329147 [D-Real: 0.5288 D-Fake: 0.4706]\n",
            "Epoch 099 | Avg Losses >>G/D 0.7870/1.327319 [D-Real: 0.5303 D-Fake: 0.4723]\n",
            "Epoch 100 | Avg Losses >>G/D 0.8021/1.320789 [D-Real: 0.5326 D-Fake: 0.4671]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_TuQmHZu2n0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}