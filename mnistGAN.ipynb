{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5vgBlkSUbPW3DAIW8Ju00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtroyv/generative-adversarial-networks/blob/main/mnistGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uENlF0Uw2N0n",
        "outputId": "b2d0a4fc-f02e-4939-944a-78c9dcf89655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "GPU Available:  True\n",
            "cuda:0\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"GPU Available: \", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device= torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device=\"cpu\"\n",
        "print(device)\n",
        "\n",
        "# If you want to save the model to your personal google drive or transfer files you need to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to create out generator\n",
        "def make_generator_network(input_size=20, num_hidden_layers=1, num_hidden_units=100, num_output_units=784):\n",
        "  model = nn.Sequential()\n",
        "  for i in range(num_hidden_layers):\n",
        "    model.add_module(f'fc_g{i}', nn.Linear(input_size, num_hidden_units))\n",
        "    model.add_module(f'relu_g{i}', nn.LeakyReLU())\n",
        "    input_size = num_hidden_units\n",
        "  model.add_module(f'fc_g{num_hidden_layers}', nn.Linear(input_size, num_output_units))\n",
        "  model.add_module('tanh_g', nn.Tanh())\n",
        "  return model\n",
        "\n",
        "#Define a function for the descriminator\n",
        "def make_descriminator_network(input_size, num_hidden_layers=1, num_hidden_units=100, num_output_units =1):\n",
        "  model = nn.Sequential()\n",
        "  for i in range(num_hidden_layers):\n",
        "    model.add_module(f'fc_d{i}', nn.Linear(input_size, num_hidden_units, bias=False))\n",
        "    model.add_module(f'relu_d{i}', nn.LeakyReLU())\n",
        "    model.add_module(f'dropout', nn.Dropout(p=0.5))\n",
        "    input_size= num_hidden_units\n",
        "  model.add_module(f'fc_d{num_hidden_layers}', nn.Linear(input_size, num_output_units))\n",
        "  model.add_module('sigmoid', nn.Sigmoid())\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "#Next we initialize each of the networks\n",
        "image_size= (28,28)\n",
        "z_size = 20\n",
        "gen_hidden_layers = 1\n",
        "gen_hidden_size = 100\n",
        "disc_hidden_layers =1\n",
        "disc_hidden_size=100\n",
        "torch.manual_seed(1)\n",
        "\n",
        "gen_model = make_generator_network(input_size=z_size, num_hidden_layers=gen_hidden_layers, num_hidden_units=gen_hidden_size, num_output_units=np.prod(image_size))\n",
        "print(gen_model)\n",
        "\n",
        "disc_model = make_descriminator_network(input_size=np.prod(image_size), num_hidden_layers=disc_hidden_layers, num_hidden_units=disc_hidden_size)\n",
        "print(disc_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FddazO2E-Gno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65a3013-b5f9-491c-d75f-4100d1655eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (fc_g0): Linear(in_features=20, out_features=100, bias=True)\n",
            "  (relu_g0): LeakyReLU(negative_slope=0.01)\n",
            "  (fc_g1): Linear(in_features=100, out_features=784, bias=True)\n",
            "  (tanh_g): Tanh()\n",
            ")\n",
            "Sequential(\n",
            "  (fc_d0): Linear(in_features=784, out_features=100, bias=False)\n",
            "  (relu_d0): LeakyReLU(negative_slope=0.01)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc_d1): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "image_path='./'\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5), std=(0.5)) ])\n",
        "\n",
        "mnist_dataset= torchvision.datasets.MNIST(root=image_path, train=True, transform = transform, download=True)\n",
        "\n",
        "example,label = next(iter(mnist_dataset))\n",
        "print(f'Min: {example.min()} Max: {example.max()}')\n",
        "print(example.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSEmUPczZUbS",
        "outputId": "129318e6-0793-4847-88f9-6d728e21f57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 484kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.83MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.09MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: -1.0 Max: 1.0\n",
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random vector z based on desired distribution\n",
        "\n",
        "def create_noise(batch_size, z_size, mode_z):\n",
        "  if mode_z =='uniform':\n",
        "    input_z = torch.rand(batch_size, z_size) *2 -1 #why do we combine it like this?\n",
        "  elif mode_z == 'normal':\n",
        "    input_z = torch.randn(batch_size, z_size)\n",
        "  return input_z"
      ],
      "metadata": {
        "id": "dJb547FAh2UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run through and get a batch of probabilities for the fake images and real images\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(mnist_dataset, batch_size, shuffle=False)\n",
        "input_real, label = next(iter(dataloader))\n",
        "input_real = input_real.view(batch_size, -1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "mode_z ='uniform'\n",
        "input_z = create_noise(batch_size, z_size, mode_z)\n",
        "print('input-z -- shape:', input_z.shape)\n",
        "print('input-real -- shape:', input_real.shape)\n",
        "\n",
        "g_output = gen_model(input_z)\n",
        "print('Output of G -- shape:', g_output.shape)\n",
        "\n",
        "d_proba_real = disc_model(input_real)\n",
        "d_proba_fake = disc_model(g_output)\n",
        "print('Probability of real images -- shape:', d_proba_real.shape)\n",
        "print('Probability of fake images -- shape:', d_proba_fake.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPqKnE6Sml5W",
        "outputId": "cc211902-69de-4f90-c093-42dd87dbcfcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input-z -- shape: torch.Size([32, 20])\n",
            "input-real -- shape: torch.Size([32, 784])\n",
            "Output of G -- shape: torch.Size([32, 784])\n",
            "Probability of real images -- shape: torch.Size([32, 1])\n",
            "Probability of fake images -- shape: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create loss and generate ground truth labels\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "# loss for generator\n",
        "g_labels_real = torch.ones_like(d_proba_fake)\n",
        "g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "print(f'Generator Loss: {g_loss:.4f}')\n",
        "\n",
        "# Loss for discriminator\n",
        "d_labels_real = torch.ones_like(d_proba_real)\n",
        "d_labels_fake = torch.zeros_like(d_proba_fake)\n",
        "d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "print(f'Discriminator losses: Real {d_loss_real:.4f} Fake {d_loss_fake:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ-hN9y_80Dd",
        "outputId": "83f1b50e-29a6-415f-f255-300ef43f0ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.6983\n",
            "Discriminator losses: Real 0.7479 Fake 0.6885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we set up teh dataloader for the real dataset, the generator, discriminator and separate adam optimizer\n",
        "\n",
        "batch_size = 64\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "mnist_dl =DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "first_images, first_labels = next(iter(mnist_dl))\n",
        "print(f'The first image is {first_images[0].shape}')\n",
        "print(f\"THe first label is {first_labels[0]}\")\n",
        "\n",
        "gen_model = make_generator_network(\n",
        "    input_size=z_size,\n",
        "    num_hidden_layers=gen_hidden_layers,\n",
        "    num_hidden_units= gen_hidden_size,\n",
        "    num_output_units=np.prod(image_size)\n",
        ").to(device)\n",
        "\n",
        "disc_model = make_descriminator_network(\n",
        "    input_size=np.prod(image_size),\n",
        "    num_hidden_layers=disc_hidden_layers,\n",
        "    num_hidden_units=disc_hidden_size\n",
        ").to(device)\n",
        "\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "g_optimizer= torch.optim.Adam(gen_model.parameters())\n",
        "d_optimizer=torch.optim.Adam(disc_model.parameters())\n",
        "\n"
      ],
      "metadata": {
        "id": "Ze32xti_FI1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921085a7-3b1a-4368-9caf-aab920490440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first image is torch.Size([1, 28, 28])\n",
            "THe first label is 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write two training utility functions for the  discriminator and generator using two separate adam optimizers\n",
        "\n",
        "# Train the discriminator\n",
        "def d_train(x):\n",
        "  disc_model.zero_grad()\n",
        "  # Train the discriminator with a real batch\n",
        "  batch_size = x.size(0)\n",
        "  x = x.view(batch_size, -1).to(device)\n",
        "  d_labels_real = torch.ones(batch_size, 1, device=device)\n",
        "  d_proba_real = disc_model(x)\n",
        "  d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "\n",
        "  # Train discriminator on a fake batch\n",
        "\n",
        "  input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "  g_output = gen_model(input_z)\n",
        "  d_proba_fake = disc_model(g_output)\n",
        "  d_labels_fake = torch.zeros(batch_size,1,device=device)\n",
        "  d_loss_fake =loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "  # Gradient backprop & optimize only D's parameters\n",
        "  d_loss = d_loss_real + d_loss_fake\n",
        "  d_loss.backward()\n",
        "  d_optimizer.step()\n",
        "  return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()\n",
        "\n",
        "  #Train the generator\n",
        "\n",
        "def g_train(x):\n",
        "  gen_model.zero_grad()\n",
        "  batch_size = x.size(0)\n",
        "  input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "  g_labels_real = torch.ones(batch_size,1, device=device)\n",
        "\n",
        "  g_output= gen_model(input_z)\n",
        "  d_proba_fake = disc_model(g_output)\n",
        "  g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "  # gradient backprop & optimize ONLY G's parameters\n",
        "  g_loss.backward()\n",
        "  g_optimizer.step()\n",
        "  return g_loss.data.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vzRUsWtuGiIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will start alternatinig traing of geneator and discriminator over 100 epochs\n",
        "# for each epoch we will record the loss for the generator, discriminator, aswell as loss for real & fake data respectively.\n",
        "# We will also generatre some examples from a the latent vector z from the current generator model via create_samples() and store these synthnesized images in a list\n",
        "\n",
        "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "  g_output = g_model(input_z)\n",
        "  images =torch.reshape(g_output, (batch_size, *image_size))\n",
        "  return (images+1)/2.0\n",
        "\n",
        "epoch_samples = []\n",
        "all_d_losses = []\n",
        "all_g_losses = []\n",
        "all_d_real = []\n",
        "all_d_fake = []\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  d_losses, g_losses = [],[]\n",
        "\n",
        "  d_vals_real, d_vals_fake = [],[]\n",
        "\n",
        "  for i, (x,_) in enumerate(mnist_dl):\n",
        "    d_loss, d_proba_real, d_proba_fake= d_train(x)\n",
        "    d_losses.append(d_loss)\n",
        "    g_losses.append(g_train(x))\n",
        "    d_vals_real.append(d_proba_real.mean().cpu())\n",
        "    d_vals_fake.append(d_proba_fake.mean().cpu())\n",
        "\n",
        "  all_d_losses.append(torch.tensor(d_losses).mean())\n",
        "  all_g_losses.append(torch.tensor(g_losses).mean())\n",
        "  all_d_real.append(torch.tensor(d_vals_real).mean())\n",
        "  all_d_fake.append(torch.tensor(d_vals_fake).mean())\n",
        "  print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
        "  f'G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:4f}'\n",
        "  f' [D-Real: {all_d_real[-1]:.4f}'\n",
        "  f' D-Fake: {all_d_fake[-1]:.4f}]')\n",
        "  epoch_samples.append(create_samples(gen_model, fixed_z).detach().cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsrJTDjq1Fz5",
        "outputId": "5101d0fe-b722-403a-8d07-94bbb0997ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Avg Losses >>G/D 0.9020/0.880472 [D-Real: 0.8317 D-Fake: 0.4781]\n",
            "Epoch 002 | Avg Losses >>G/D 1.0007/1.092743 [D-Real: 0.6307 D-Fake: 0.4228]\n",
            "Epoch 003 | Avg Losses >>G/D 0.9675/1.160848 [D-Real: 0.5989 D-Fake: 0.4183]\n",
            "Epoch 004 | Avg Losses >>G/D 0.9796/1.194591 [D-Real: 0.5832 D-Fake: 0.4180]\n",
            "Epoch 005 | Avg Losses >>G/D 0.9081/1.233730 [D-Real: 0.5686 D-Fake: 0.4323]\n",
            "Epoch 006 | Avg Losses >>G/D 0.9623/1.215204 [D-Real: 0.5723 D-Fake: 0.4205]\n",
            "Epoch 007 | Avg Losses >>G/D 0.8827/1.266847 [D-Real: 0.5527 D-Fake: 0.4431]\n",
            "Epoch 008 | Avg Losses >>G/D 1.0358/1.180521 [D-Real: 0.5917 D-Fake: 0.4089]\n",
            "Epoch 009 | Avg Losses >>G/D 1.1428/1.095757 [D-Real: 0.6252 D-Fake: 0.3808]\n",
            "Epoch 010 | Avg Losses >>G/D 0.9542/1.197297 [D-Real: 0.5876 D-Fake: 0.4184]\n",
            "Epoch 011 | Avg Losses >>G/D 0.9545/1.205703 [D-Real: 0.5836 D-Fake: 0.4190]\n",
            "Epoch 012 | Avg Losses >>G/D 0.9768/1.206133 [D-Real: 0.5817 D-Fake: 0.4169]\n",
            "Epoch 013 | Avg Losses >>G/D 0.9669/1.204329 [D-Real: 0.5821 D-Fake: 0.4180]\n",
            "Epoch 014 | Avg Losses >>G/D 0.9616/1.207925 [D-Real: 0.5826 D-Fake: 0.4199]\n",
            "Epoch 015 | Avg Losses >>G/D 0.9219/1.237280 [D-Real: 0.5698 D-Fake: 0.4308]\n",
            "Epoch 016 | Avg Losses >>G/D 0.9098/1.241003 [D-Real: 0.5684 D-Fake: 0.4352]\n",
            "Epoch 017 | Avg Losses >>G/D 0.8867/1.260598 [D-Real: 0.5596 D-Fake: 0.4406]\n",
            "Epoch 018 | Avg Losses >>G/D 0.8449/1.292415 [D-Real: 0.5451 D-Fake: 0.4527]\n",
            "Epoch 019 | Avg Losses >>G/D 0.8154/1.307110 [D-Real: 0.5378 D-Fake: 0.4605]\n",
            "Epoch 020 | Avg Losses >>G/D 0.8250/1.311625 [D-Real: 0.5365 D-Fake: 0.4611]\n",
            "Epoch 021 | Avg Losses >>G/D 0.8424/1.291331 [D-Real: 0.5445 D-Fake: 0.4536]\n",
            "Epoch 022 | Avg Losses >>G/D 0.8433/1.291119 [D-Real: 0.5473 D-Fake: 0.4556]\n",
            "Epoch 023 | Avg Losses >>G/D 0.8527/1.280839 [D-Real: 0.5499 D-Fake: 0.4511]\n",
            "Epoch 024 | Avg Losses >>G/D 0.8169/1.306358 [D-Real: 0.5387 D-Fake: 0.4611]\n",
            "Epoch 025 | Avg Losses >>G/D 0.8397/1.294256 [D-Real: 0.5458 D-Fake: 0.4563]\n",
            "Epoch 026 | Avg Losses >>G/D 0.7983/1.317690 [D-Real: 0.5341 D-Fake: 0.4666]\n",
            "Epoch 027 | Avg Losses >>G/D 0.8025/1.321376 [D-Real: 0.5318 D-Fake: 0.4660]\n",
            "Epoch 028 | Avg Losses >>G/D 0.8109/1.311642 [D-Real: 0.5361 D-Fake: 0.4635]\n",
            "Epoch 029 | Avg Losses >>G/D 0.8415/1.290754 [D-Real: 0.5460 D-Fake: 0.4550]\n",
            "Epoch 030 | Avg Losses >>G/D 0.8239/1.300653 [D-Real: 0.5423 D-Fake: 0.4593]\n",
            "Epoch 031 | Avg Losses >>G/D 0.8009/1.314518 [D-Real: 0.5348 D-Fake: 0.4652]\n",
            "Epoch 032 | Avg Losses >>G/D 0.8160/1.308235 [D-Real: 0.5386 D-Fake: 0.4613]\n",
            "Epoch 033 | Avg Losses >>G/D 0.7959/1.321635 [D-Real: 0.5322 D-Fake: 0.4678]\n",
            "Epoch 034 | Avg Losses >>G/D 0.7896/1.332059 [D-Real: 0.5273 D-Fake: 0.4713]\n",
            "Epoch 035 | Avg Losses >>G/D 0.8106/1.315635 [D-Real: 0.5345 D-Fake: 0.4647]\n",
            "Epoch 036 | Avg Losses >>G/D 0.7831/1.327964 [D-Real: 0.5290 D-Fake: 0.4714]\n",
            "Epoch 037 | Avg Losses >>G/D 0.7855/1.326026 [D-Real: 0.5297 D-Fake: 0.4697]\n",
            "Epoch 038 | Avg Losses >>G/D 0.8040/1.310871 [D-Real: 0.5378 D-Fake: 0.4654]\n",
            "Epoch 039 | Avg Losses >>G/D 0.8120/1.309466 [D-Real: 0.5389 D-Fake: 0.4628]\n",
            "Epoch 040 | Avg Losses >>G/D 0.8026/1.326465 [D-Real: 0.5301 D-Fake: 0.4664]\n",
            "Epoch 041 | Avg Losses >>G/D 0.7780/1.334654 [D-Real: 0.5252 D-Fake: 0.4731]\n",
            "Epoch 042 | Avg Losses >>G/D 0.8033/1.314497 [D-Real: 0.5357 D-Fake: 0.4653]\n",
            "Epoch 043 | Avg Losses >>G/D 0.8026/1.313767 [D-Real: 0.5356 D-Fake: 0.4646]\n",
            "Epoch 044 | Avg Losses >>G/D 0.8086/1.315317 [D-Real: 0.5341 D-Fake: 0.4641]\n",
            "Epoch 045 | Avg Losses >>G/D 0.8288/1.298011 [D-Real: 0.5439 D-Fake: 0.4597]\n",
            "Epoch 046 | Avg Losses >>G/D 0.8160/1.301834 [D-Real: 0.5412 D-Fake: 0.4608]\n",
            "Epoch 047 | Avg Losses >>G/D 0.8215/1.300598 [D-Real: 0.5419 D-Fake: 0.4586]\n",
            "Epoch 048 | Avg Losses >>G/D 0.8349/1.297420 [D-Real: 0.5440 D-Fake: 0.4570]\n",
            "Epoch 049 | Avg Losses >>G/D 0.7952/1.323006 [D-Real: 0.5317 D-Fake: 0.4674]\n",
            "Epoch 050 | Avg Losses >>G/D 0.8009/1.311454 [D-Real: 0.5365 D-Fake: 0.4653]\n",
            "Epoch 051 | Avg Losses >>G/D 0.8077/1.310214 [D-Real: 0.5374 D-Fake: 0.4637]\n",
            "Epoch 052 | Avg Losses >>G/D 0.7940/1.322573 [D-Real: 0.5325 D-Fake: 0.4691]\n",
            "Epoch 053 | Avg Losses >>G/D 0.7948/1.324852 [D-Real: 0.5305 D-Fake: 0.4678]\n",
            "Epoch 054 | Avg Losses >>G/D 0.8106/1.312091 [D-Real: 0.5364 D-Fake: 0.4639]\n",
            "Epoch 055 | Avg Losses >>G/D 0.7833/1.332234 [D-Real: 0.5270 D-Fake: 0.4718]\n",
            "Epoch 056 | Avg Losses >>G/D 0.7977/1.321148 [D-Real: 0.5321 D-Fake: 0.4662]\n",
            "Epoch 057 | Avg Losses >>G/D 0.7847/1.326144 [D-Real: 0.5304 D-Fake: 0.4712]\n",
            "Epoch 058 | Avg Losses >>G/D 0.7927/1.322942 [D-Real: 0.5319 D-Fake: 0.4692]\n",
            "Epoch 059 | Avg Losses >>G/D 0.7961/1.322255 [D-Real: 0.5317 D-Fake: 0.4680]\n",
            "Epoch 060 | Avg Losses >>G/D 0.7842/1.330835 [D-Real: 0.5275 D-Fake: 0.4712]\n",
            "Epoch 061 | Avg Losses >>G/D 0.8013/1.317211 [D-Real: 0.5343 D-Fake: 0.4667]\n",
            "Epoch 062 | Avg Losses >>G/D 0.7845/1.329724 [D-Real: 0.5289 D-Fake: 0.4710]\n",
            "Epoch 063 | Avg Losses >>G/D 0.8010/1.315999 [D-Real: 0.5349 D-Fake: 0.4664]\n",
            "Epoch 064 | Avg Losses >>G/D 0.8036/1.312836 [D-Real: 0.5355 D-Fake: 0.4634]\n",
            "Epoch 065 | Avg Losses >>G/D 0.8013/1.323689 [D-Real: 0.5315 D-Fake: 0.4678]\n",
            "Epoch 066 | Avg Losses >>G/D 0.8112/1.312955 [D-Real: 0.5364 D-Fake: 0.4645]\n",
            "Epoch 067 | Avg Losses >>G/D 0.8129/1.312925 [D-Real: 0.5362 D-Fake: 0.4642]\n",
            "Epoch 068 | Avg Losses >>G/D 0.7922/1.328716 [D-Real: 0.5283 D-Fake: 0.4693]\n",
            "Epoch 069 | Avg Losses >>G/D 0.7882/1.329147 [D-Real: 0.5284 D-Fake: 0.4704]\n",
            "Epoch 070 | Avg Losses >>G/D 0.7878/1.331537 [D-Real: 0.5273 D-Fake: 0.4701]\n",
            "Epoch 071 | Avg Losses >>G/D 0.7813/1.330243 [D-Real: 0.5283 D-Fake: 0.4718]\n",
            "Epoch 072 | Avg Losses >>G/D 0.7803/1.334838 [D-Real: 0.5271 D-Fake: 0.4732]\n",
            "Epoch 073 | Avg Losses >>G/D 0.7713/1.340612 [D-Real: 0.5233 D-Fake: 0.4757]\n",
            "Epoch 074 | Avg Losses >>G/D 0.7807/1.335282 [D-Real: 0.5268 D-Fake: 0.4733]\n",
            "Epoch 075 | Avg Losses >>G/D 0.7722/1.337937 [D-Real: 0.5245 D-Fake: 0.4751]\n",
            "Epoch 076 | Avg Losses >>G/D 0.7732/1.337813 [D-Real: 0.5252 D-Fake: 0.4758]\n",
            "Epoch 077 | Avg Losses >>G/D 0.7742/1.342234 [D-Real: 0.5228 D-Fake: 0.4767]\n",
            "Epoch 078 | Avg Losses >>G/D 0.7951/1.324303 [D-Real: 0.5323 D-Fake: 0.4707]\n",
            "Epoch 079 | Avg Losses >>G/D 0.7737/1.326571 [D-Real: 0.5302 D-Fake: 0.4739]\n",
            "Epoch 080 | Avg Losses >>G/D 0.7886/1.331621 [D-Real: 0.5276 D-Fake: 0.4714]\n",
            "Epoch 081 | Avg Losses >>G/D 0.7887/1.328805 [D-Real: 0.5295 D-Fake: 0.4709]\n",
            "Epoch 082 | Avg Losses >>G/D 0.7637/1.343893 [D-Real: 0.5214 D-Fake: 0.4773]\n",
            "Epoch 083 | Avg Losses >>G/D 0.7703/1.345306 [D-Real: 0.5215 D-Fake: 0.4770]\n",
            "Epoch 084 | Avg Losses >>G/D 0.7642/1.342408 [D-Real: 0.5222 D-Fake: 0.4773]\n",
            "Epoch 085 | Avg Losses >>G/D 0.7710/1.339739 [D-Real: 0.5239 D-Fake: 0.4756]\n",
            "Epoch 086 | Avg Losses >>G/D 0.7647/1.342160 [D-Real: 0.5228 D-Fake: 0.4783]\n",
            "Epoch 087 | Avg Losses >>G/D 0.7780/1.337825 [D-Real: 0.5248 D-Fake: 0.4740]\n",
            "Epoch 088 | Avg Losses >>G/D 0.7936/1.327857 [D-Real: 0.5301 D-Fake: 0.4713]\n",
            "Epoch 089 | Avg Losses >>G/D 0.7940/1.325294 [D-Real: 0.5307 D-Fake: 0.4697]\n",
            "Epoch 090 | Avg Losses >>G/D 0.7721/1.337868 [D-Real: 0.5236 D-Fake: 0.4748]\n",
            "Epoch 091 | Avg Losses >>G/D 0.7794/1.337672 [D-Real: 0.5253 D-Fake: 0.4738]\n",
            "Epoch 092 | Avg Losses >>G/D 0.7713/1.339555 [D-Real: 0.5240 D-Fake: 0.4761]\n",
            "Epoch 093 | Avg Losses >>G/D 0.7868/1.331999 [D-Real: 0.5289 D-Fake: 0.4728]\n",
            "Epoch 094 | Avg Losses >>G/D 0.7696/1.344829 [D-Real: 0.5223 D-Fake: 0.4776]\n",
            "Epoch 095 | Avg Losses >>G/D 0.7708/1.338978 [D-Real: 0.5244 D-Fake: 0.4761]\n",
            "Epoch 096 | Avg Losses >>G/D 0.7663/1.343530 [D-Real: 0.5227 D-Fake: 0.4779]\n",
            "Epoch 097 | Avg Losses >>G/D 0.7905/1.329365 [D-Real: 0.5293 D-Fake: 0.4711]\n",
            "Epoch 098 | Avg Losses >>G/D 0.7904/1.329147 [D-Real: 0.5288 D-Fake: 0.4706]\n",
            "Epoch 099 | Avg Losses >>G/D 0.7870/1.327319 [D-Real: 0.5303 D-Fake: 0.4723]\n",
            "Epoch 100 | Avg Losses >>G/D 0.8021/1.320789 [D-Real: 0.5326 D-Fake: 0.4671]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in this cel lets plot the convergence of probabilities as well as some examples\n",
        "# of the synthesized images"
      ],
      "metadata": {
        "id": "vdO2ZomS0BvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will attempt to improve the our previous results from GAN via an implementation following the\n",
        "# CGAN architecture. The main differences will layers using transposed Convolution, aswell as applying BatchNorm to standardize features\n",
        "# and enable smoother training\n",
        "\n",
        "# recreate the make_generator_network funciton\n",
        "def make_generator_network(input_size, n_filters):\n",
        "  model = nn.Sequential(\n",
        "      nn.ConvTranspose2d(input_size, n_filters*4, 4,1,0, bias=False),\n",
        "      nn.BatchNorm2d(n_filters*4),\n",
        "      nn.LeakyReLU(0.2),\n",
        "      nn.ConvTranspose2d(n_filters*4, n_filters*2, 3,2,1, bias=False),\n",
        "      nn.BatchNorm2d(n_filters*2),\n",
        "      nn.LeakyReLU(0.2),\n",
        "      nn.ConvTranspose2d(n_filters*2, n_filters, 4,2,1, bias=False),\n",
        "      nn.BatchNorm2d(n_filters),\n",
        "      nn.LeakyReLU(0.2),\n",
        "      nn.ConvTranspose2d(n_filters, 1, 4,2,1, bias=False),\n",
        "      nn.Tanh()\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, n_filters):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Conv2d(1, n_filters, 4,2,1, bias=False),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(n_filters, n_filters*2, 4,2,1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(n_filters*2, n_filters*4, 3,2,1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(n_filters*4, 1, 4,1,0, bias=False),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self,input):\n",
        "    output = self.network(input)\n",
        "    return output.view(-1,1).squeeze(0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g_TuQmHZu2n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_size = 100\n",
        "image_size = (28,28)\n",
        "n_filters=32\n",
        "\n",
        "# Lets create the gen_model and disc_model and see it's architecture\n",
        "gen_model= make_generator_network(z_size, n_filters).to(device)\n",
        "print(gen_model)\n",
        "\n",
        "disc_model = Discriminator(n_filters).to(device)\n",
        "print(disc_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwXx9AbFxuBv",
        "outputId": "085d5b47-9250-41d5-dc50-1a68c3934761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): LeakyReLU(negative_slope=0.2)\n",
            "  (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): LeakyReLU(negative_slope=0.2)\n",
            "  (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): LeakyReLU(negative_slope=0.2)\n",
            "  (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (10): Tanh()\n",
            ")\n",
            "Discriminator(\n",
            "  (network): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.003)\n",
        "d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.002)"
      ],
      "metadata": {
        "id": "z7fT_tNlAGl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets rewrite the create_noise function so it outputs a tensor of four dimensions instead of a vector\n",
        "\n",
        "def create_noise(batch_size, z_size, mode_z):\n",
        "  if mode_z == 'uniform':\n",
        "    input_z = torch.rand(batch_size, z_size,1,1)*1 -1\n",
        "  elif mode_z == 'normal':\n",
        "    input_z = torch.randn(batch_size, z_size,1,1)\n",
        "  return input_z\n",
        "\n",
        "\n",
        "# now we rewrite a d_train() funciton for training the discriminator it doesn't need to reshape the input image\n",
        "\n",
        "def d_train(x):\n",
        "  disc_model.zero_grad()\n",
        "\n",
        "  # train discriminator with a real batch\n",
        "  batch_size = x.size(0)\n",
        "  x = x.to(device)\n",
        "  d_labels_real = torch.ones(batch_size,1,device=device)\n",
        "  d_proba_real = disc_model(x)\n",
        "  d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "\n",
        "  # Train discriminator on a fake batch\n",
        "  input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "  g_output = gen_model(input_z)\n",
        "  d_proba_fake = disc_model(g_output)\n",
        "  d_labels_fake= torch.zeros(batch_size,1,device=device)\n",
        "  d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "  # gradient backprop & optimize only D's parameters\n",
        "  d_loss = d_loss_real + d_loss_fake\n",
        "  d_loss.backward()\n",
        "  d_optimizer.step()\n",
        "  return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()\n"
      ],
      "metadata": {
        "id": "l4ZwTus2Aqec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we alternate between the training of the generator and the discriminator for 100 epochs\n",
        "# we will also generate some samples aswell\n",
        "\n",
        "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "epoch_samples= []\n",
        "torch.manual_seed(1)\n",
        "epoch_samples = []\n",
        "all_d_losses = []\n",
        "all_g_losses = []\n",
        "all_d_real = []\n",
        "all_d_fake = []\n",
        "num_epochs = 100\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "  g_output = g_model(input_z)\n",
        "  images =torch.reshape(g_output, (batch_size, *image_size))\n",
        "  return (images+1)/2.0\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  d_losses, g_losses = [],[]\n",
        "  gen_model.train()\n",
        "\n",
        "  for i, (x,_) in enumerate(mnist_dl):\n",
        "    d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
        "    d_losses.append(d_loss)\n",
        "    g_losses.append(g_train(x))\n",
        "\n",
        "  print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
        "        f' G/D {torch.FloatTensor(g_losses).mean():.4f}'\n",
        "        f'/{torch.FloatTensor(d_losses).mean():.4f}')\n",
        "\n",
        "  gen_model.eval()\n",
        "  epoch_samples.append(create_samples(gen_model, fixed_z).detach().cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOdVnie2EBzL",
        "outputId": "0635a39c-8310-4517-bc12-4cdf75561593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Avg Losses >> G/D 8.3308/0.0462\n",
            "Epoch 002 | Avg Losses >> G/D 9.3807/0.0484\n",
            "Epoch 003 | Avg Losses >> G/D 9.0330/0.0287\n",
            "Epoch 004 | Avg Losses >> G/D 10.3196/0.0006\n",
            "Epoch 005 | Avg Losses >> G/D 37.8093/0.0089\n",
            "Epoch 006 | Avg Losses >> G/D 55.3489/0.0000\n",
            "Epoch 007 | Avg Losses >> G/D 55.3483/0.0000\n",
            "Epoch 008 | Avg Losses >> G/D 55.3485/0.0000\n",
            "Epoch 009 | Avg Losses >> G/D 55.3483/0.0000\n",
            "Epoch 010 | Avg Losses >> G/D 55.3485/0.0000\n",
            "Epoch 011 | Avg Losses >> G/D 55.3484/0.0000\n",
            "Epoch 012 | Avg Losses >> G/D 55.3487/0.0000\n",
            "Epoch 013 | Avg Losses >> G/D 55.3483/0.0000\n",
            "Epoch 014 | Avg Losses >> G/D 55.3487/0.0000\n",
            "Epoch 015 | Avg Losses >> G/D 55.3488/0.0000\n",
            "Epoch 016 | Avg Losses >> G/D 55.3488/0.0000\n",
            "Epoch 017 | Avg Losses >> G/D 55.3481/0.0000\n",
            "Epoch 018 | Avg Losses >> G/D 55.3486/0.0000\n",
            "Epoch 019 | Avg Losses >> G/D 55.3486/0.0000\n",
            "Epoch 020 | Avg Losses >> G/D 55.3482/0.0000\n",
            "Epoch 021 | Avg Losses >> G/D 55.3486/0.0000\n",
            "Epoch 022 | Avg Losses >> G/D 55.3485/0.0000\n",
            "Epoch 023 | Avg Losses >> G/D 55.3484/0.0000\n",
            "Epoch 024 | Avg Losses >> G/D 55.3480/0.0000\n",
            "Epoch 025 | Avg Losses >> G/D 55.3483/0.0000\n",
            "Epoch 026 | Avg Losses >> G/D 55.3478/0.0000\n",
            "Epoch 027 | Avg Losses >> G/D 55.3491/0.0000\n",
            "Epoch 028 | Avg Losses >> G/D 55.3485/0.0000\n",
            "Epoch 029 | Avg Losses >> G/D 55.3484/0.0000\n",
            "Epoch 030 | Avg Losses >> G/D 55.3479/0.0000\n",
            "Epoch 031 | Avg Losses >> G/D 55.3485/0.0000\n",
            "Epoch 032 | Avg Losses >> G/D 55.3485/0.0000\n",
            "Epoch 033 | Avg Losses >> G/D 55.3478/0.0000\n",
            "Epoch 034 | Avg Losses >> G/D 55.3478/0.0000\n",
            "Epoch 035 | Avg Losses >> G/D 55.3472/0.0000\n",
            "Epoch 036 | Avg Losses >> G/D 55.3460/0.0000\n",
            "Epoch 037 | Avg Losses >> G/D 55.3443/0.0000\n",
            "Epoch 038 | Avg Losses >> G/D 55.3420/0.0000\n",
            "Epoch 039 | Avg Losses >> G/D 55.3375/0.0000\n",
            "Epoch 040 | Avg Losses >> G/D 55.3317/0.0000\n",
            "Epoch 041 | Avg Losses >> G/D 55.3238/0.0000\n",
            "Epoch 042 | Avg Losses >> G/D 55.3137/0.0000\n",
            "Epoch 043 | Avg Losses >> G/D 55.3013/0.0000\n",
            "Epoch 044 | Avg Losses >> G/D 55.2859/0.0000\n",
            "Epoch 045 | Avg Losses >> G/D 55.2680/0.0000\n",
            "Epoch 046 | Avg Losses >> G/D 55.2486/0.0000\n",
            "Epoch 047 | Avg Losses >> G/D 55.2266/0.0000\n",
            "Epoch 048 | Avg Losses >> G/D 55.2023/0.0000\n",
            "Epoch 049 | Avg Losses >> G/D 55.1753/0.0000\n",
            "Epoch 050 | Avg Losses >> G/D 55.1457/0.0000\n",
            "Epoch 051 | Avg Losses >> G/D 55.1150/0.0000\n",
            "Epoch 052 | Avg Losses >> G/D 55.0818/0.0000\n",
            "Epoch 053 | Avg Losses >> G/D 55.0457/0.0000\n",
            "Epoch 054 | Avg Losses >> G/D 55.0079/0.0000\n",
            "Epoch 055 | Avg Losses >> G/D 54.9664/0.0000\n",
            "Epoch 056 | Avg Losses >> G/D 54.9214/0.0000\n",
            "Epoch 057 | Avg Losses >> G/D 54.8711/0.0000\n",
            "Epoch 058 | Avg Losses >> G/D 54.8158/0.0000\n",
            "Epoch 059 | Avg Losses >> G/D 54.7528/0.0000\n",
            "Epoch 060 | Avg Losses >> G/D 54.6795/0.0000\n",
            "Epoch 061 | Avg Losses >> G/D 54.5923/0.0000\n",
            "Epoch 062 | Avg Losses >> G/D 54.4835/0.0000\n",
            "Epoch 063 | Avg Losses >> G/D 54.3358/0.0000\n",
            "Epoch 064 | Avg Losses >> G/D 54.1163/0.0000\n",
            "Epoch 065 | Avg Losses >> G/D 53.7202/0.0000\n",
            "Epoch 066 | Avg Losses >> G/D 52.3191/0.0000\n",
            "Epoch 067 | Avg Losses >> G/D 59.3005/0.4924\n",
            "Epoch 068 | Avg Losses >> G/D 85.6553/0.0002\n",
            "Epoch 069 | Avg Losses >> G/D 84.7360/0.0000\n",
            "Epoch 070 | Avg Losses >> G/D 84.7268/0.0000\n",
            "Epoch 071 | Avg Losses >> G/D 84.7225/0.0000\n",
            "Epoch 072 | Avg Losses >> G/D 84.7147/0.0000\n",
            "Epoch 073 | Avg Losses >> G/D 84.7065/0.0000\n",
            "Epoch 074 | Avg Losses >> G/D 84.6978/0.0000\n",
            "Epoch 075 | Avg Losses >> G/D 84.6853/0.0000\n",
            "Epoch 076 | Avg Losses >> G/D 84.6697/0.0000\n",
            "Epoch 077 | Avg Losses >> G/D 84.6340/0.0000\n",
            "Epoch 078 | Avg Losses >> G/D 84.4776/0.0000\n",
            "Epoch 079 | Avg Losses >> G/D 84.4595/0.0000\n",
            "Epoch 080 | Avg Losses >> G/D 84.4312/0.0000\n",
            "Epoch 081 | Avg Losses >> G/D 84.3565/0.0000\n",
            "Epoch 082 | Avg Losses >> G/D 84.2765/0.0000\n",
            "Epoch 083 | Avg Losses >> G/D 84.2078/0.0000\n",
            "Epoch 084 | Avg Losses >> G/D 84.0979/0.0000\n",
            "Epoch 085 | Avg Losses >> G/D 83.8035/0.0000\n",
            "Epoch 086 | Avg Losses >> G/D 83.5599/0.0000\n",
            "Epoch 087 | Avg Losses >> G/D 83.5355/0.0000\n",
            "Epoch 088 | Avg Losses >> G/D 83.4883/0.0000\n",
            "Epoch 089 | Avg Losses >> G/D 83.4302/0.0000\n",
            "Epoch 090 | Avg Losses >> G/D 83.3782/0.0000\n",
            "Epoch 091 | Avg Losses >> G/D 83.2881/0.0000\n",
            "Epoch 092 | Avg Losses >> G/D 83.2026/0.0000\n",
            "Epoch 093 | Avg Losses >> G/D 83.1498/0.0000\n",
            "Epoch 094 | Avg Losses >> G/D 83.0739/0.0000\n",
            "Epoch 095 | Avg Losses >> G/D 82.9117/0.0000\n",
            "Epoch 096 | Avg Losses >> G/D 82.8320/0.0000\n",
            "Epoch 097 | Avg Losses >> G/D 82.6369/0.0000\n",
            "Epoch 098 | Avg Losses >> G/D 82.4663/0.0000\n",
            "Epoch 099 | Avg Losses >> G/D 82.2509/0.0000\n",
            "Epoch 100 | Avg Losses >> G/D 82.1887/0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets visualize the saved examples\n",
        "from matplotlib import pyplot as plt\n",
        "selected_epochs = [1,2,4,10,50,100]\n",
        "\n",
        "fig = plt.figure(figsize=(10,14))\n",
        "for i,e in enumerate(selected_epochs):\n",
        "  for j in range(5):\n",
        "    ax = fig.add_subplot(6,5,i*5+j+1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    if j ==0:\n",
        "      ax.text(-0.06, 0.5, f'Epoch{e}', rotation=90, size=18, color='red', horizontalalignment='right', verticalalignment='center', transform=ax.transAxes)\n",
        "\n",
        "    image=epoch_samples[e-1][j]\n",
        "    ax.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "A7vcHWFyNGMN",
        "outputId": "bdd8c75a-5c96-4d3e-f186-a7dcde677a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'epoch_samples' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1819552696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.06\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Epoch{e}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizontalalignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverticalalignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransAxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epoch_samples' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}